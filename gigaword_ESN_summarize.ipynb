{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ijackson/meg-decoding/decode/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ijackson/.cache/huggingface/hub/datasets--Harvard--gigaword/snapshots/e45e01b2da13842bb3df1b12dc046910147b3d82/data/ggw_data.zip'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Gigaword dataset\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_hub_download(repo_id='Harvard/gigaword', filename='data/ggw_data.zip', repo_type='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from gigaword import Gigaword\n",
    "gw = Gigaword()\n",
    "r = gw.download_and_prepare(output_dir='./output_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "train1 = pa.ipc.RecordBatchStreamReader('./output_dir/gigaword-train-00000-of-00002.arrow').read_all()\n",
    "train2 = pa.ipc.RecordBatchStreamReader('./output_dir/gigaword-train-00001-of-00002.arrow').read_all()\n",
    "train = pa.concat_tables([train1, train2])\n",
    "test = pa.ipc.RecordBatchStreamReader('./output_dir/gigaword-test.arrow').read_all()\n",
    "val = pa.ipc.RecordBatchStreamReader('./output_dir/gigaword-validation.arrow').read_all()\n",
    "\n",
    "# split into input and target\n",
    "\n",
    "train_input = [str(input_) for input_ in train['document']]\n",
    "train_target = [str(target) for target in train['summary']]\n",
    "test_input = [str(input_) for input_ in test['document']]\n",
    "test_target = [str(target) for target in test['summary']]\n",
    "val_input = [str(input_) for input_ in val['document']]\n",
    "val_target = [str(target) for target in val['summary']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a small subset of the data\n",
    "numTrain = 1000\n",
    "numTest = 100\n",
    "train_input = train_input[:numTrain]\n",
    "train_target = train_target[:numTrain]\n",
    "test_input = test_input[:numTest]\n",
    "test_target = test_target[:numTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up encodings\n",
    "train_input_encodings = []\n",
    "train_target_encodings = []\n",
    "test_input_encodings = []\n",
    "test_target_encodings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use BART to encode the training texts\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Initialize the BART model and tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "\n",
    "\n",
    "for i in range(len(train_input)):\n",
    "    x_inputs = tokenizer(train_input[i], return_tensors='pt')\n",
    "    y_inputs = tokenizer(train_target[i], return_tensors='pt')\n",
    "    encoder_outputs_x = model.get_encoder()(**x_inputs)\n",
    "    encoder_outputs_y = model.get_encoder()(**y_inputs)\n",
    "    train_input_encodings.append(encoder_outputs_x.last_hidden_state)\n",
    "    train_target_encodings.append(encoder_outputs_y.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for test texts\n",
    "for i in range(len(test_input)):\n",
    "    x_inputs = tokenizer(test_input[i], return_tensors='pt')\n",
    "    y_inputs = tokenizer(test_target[i], return_tensors='pt')\n",
    "    encoder_outputs_x = model.get_encoder()(**x_inputs)\n",
    "    encoder_outputs_y = model.get_encoder()(**y_inputs)\n",
    "    test_input_encodings.append(encoder_outputs_x.last_hidden_state)\n",
    "    test_target_encodings.append(encoder_outputs_y.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 500)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create reservoir computing model and feed the encodings as a time series\n",
    "import reservoirpy as rpy\n",
    "import numpy as np\n",
    "\n",
    "rpy.verbosity(0)\n",
    "rpy.set_seed(42)\n",
    "nNeurons = 500\n",
    "learning_rate = 0.5\n",
    "spectral_radius = 0.9\n",
    "reservoir = rpy.nodes.Reservoir(nNeurons, lr=learning_rate, sr=spectral_radius)\n",
    "res_states = []\n",
    "# get reservoir states for each brain state\n",
    "reset = True\n",
    "for token_embedding in train_input_encodings:\n",
    "   token_embedding = token_embedding.detach().numpy().squeeze()\n",
    "   timeseries_states = reservoir.run(token_embedding, reset=reset)\n",
    "   res_states.append(timeseries_states[-1]) # take the last state\n",
    "\n",
    "res_states_training = np.array(res_states)\n",
    "res_states_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need target states to be the same shape so we'll pad/truncate them as needed\n",
    "def pad_or_truncate(sequence, max_length=12): # from chatgpt\n",
    "    if len(sequence) < max_length:\n",
    "        # Pad with zeros if shorter\n",
    "        return np.pad(sequence, ((0, max_length - len(sequence)), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        # Truncate if longer\n",
    "        return sequence[:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 12, 1024)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the target encodings are the same shape\n",
    "\n",
    "train_target_encodings = np.array([pad_or_truncate(state.detach().numpy().squeeze()) for state in train_target_encodings])\n",
    "test_target_encodings = np.array([pad_or_truncate(state.detach().numpy().squeeze()) for state in test_target_encodings])\n",
    "\n",
    "train_target_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the encodings too this is probably a bad idea aha but we'll see\n",
    "train_target_encodings = train_target_encodings.reshape((train_target_encodings.shape[0], -1))\n",
    "test_target_encodings = test_target_encodings.reshape((test_target_encodings.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the readout on the training data\n",
    "#train_target_encodings = [target.detach().numpy().squeeze() for target in train_target_encodings]\n",
    "readout = rpy.nodes.Ridge(ridge=1e-7)\n",
    "readout = readout.fit(res_states_training, train_target_encodings, warmup=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_encodings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 500)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_states = []\n",
    "# get reservoir states for test states\n",
    "reset = True\n",
    "for token_embedding in test_input_encodings:\n",
    "   token_embedding = token_embedding.detach().numpy().squeeze()\n",
    "   timeseries_states = reservoir.run(token_embedding, reset=reset)\n",
    "   res_states.append(timeseries_states[-1]) # take the last state\n",
    "\n",
    "res_states_test = np.array(res_states)\n",
    "res_states_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 12288)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the readout predictions\n",
    "predictions = readout.run(res_states_test)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build base model output object from predictions\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "def build_base_model_output_from_predictions(prediction):\n",
    "    # first reshape the prediction to be the same shape as the hidden states\n",
    "    prediction = prediction.reshape((12, 1024))\n",
    "    # need to make it a batch size of 64 or else decoder gets mad\n",
    "    prediction = np.array([prediction])\n",
    "    # make it a float\n",
    "    prediction = prediction.astype('float32')\n",
    "    # now build the batch encoding object\n",
    "    bmo = BaseModelOutput(last_hidden_state=torch.tensor(prediction))\n",
    "    # attention mask should be 1s up to max_length and 0s after\n",
    "    # batch_encoding['attention_mask'] = torch.tensor([[1]*12 + [0]*38]*prediction.shape[0])\n",
    "    return bmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_predictions = [build_base_model_output_from_predictions(prediction) for prediction in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1024])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_predictions[0].last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can decode the predictions as text\n",
    "# decode a small range\n",
    "# decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
    "\n",
    "decoded_texts = []\n",
    "num_texts = 10\n",
    "start = np.random.randint(0, len(base_model_predictions) - num_texts)\n",
    "range = [start, start + 10]\n",
    "\n",
    "for prediction in base_model_predictions[range[0]:range[1]]:\n",
    "    # Generate text from the embeddings\n",
    "    try:\n",
    "        generated_ids = model.generate(decoder_input_ids=decoder_input_ids, encoder_outputs=prediction, max_length=50)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(prediction.last_hidden_state.shape)\n",
    "        print(e)\n",
    "        break\n",
    "    # Decode the generated ids to text\n",
    "    decoded_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    decoded_texts.append(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['U.YI/U.S.air, TUI, NU and N0G go in to one-U.N. for one in one, to one in three.T.I/OSN,',\n",
       " ' l k l d dia l k ke. le d d k l kto le d k k',\n",
       " '',\n",
       " 'Finance Minister of the Bank of Japan (BoT) says it will bring into an aero-mono plan of an omer-Tobel at the end of the year. e.t.m.e in',\n",
       " 'fopan gaiopan on the prowl for the UN, s.s.aopan for the United States of S.E.A.D. president of the Republic of the D.S.C.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mittal launches hostile arcelor bid in us',\n",
       " 'time not ripe yet for indian mangoes to hit us',\n",
       " '#.# billion tv viewers expected for opening world cup match',\n",
       " 'rumsfeld calls zarqawi death significant victory',\n",
       " 'french farm offers hope for endangered asian crocs UNK picture']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_target[range[0]:range[1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
